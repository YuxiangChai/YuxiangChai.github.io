---
---

@INPROCEEDINGS{9636640,
  author={Sheng*, Diwei and Chai*, Yuxiang and Li, Xinru and Feng, Chen and Lin, Jianzhe and Silva, Claudio and Rizzo, John-Ross},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences}, 
  year={2021},
  volume={},
  number={},
  pages={9773-9779},
  abstract={Visual place recognition (VPR) is critical in not only localization and mapping for autonomous driving vehicles, but also assistive navigation for the visually impaired population. To enable a long-term VPR system on a large scale, several challenges need to be addressed. First, different applications could require different image view directions, such as front views for self-driving cars while side views for the low vision people. Second, VPR in metropolitan scenes can often cause privacy concerns due to the imaging of pedestrian and vehicle identity information, calling for the need for data anonymization before VPR queries and database construction. Both factors could lead to VPR performance variations that are not well understood yet. To study their influences, we present the NYU-VPR dataset that contains more than 200,000 images over a 2km√ó2km area near the New York University campus, taken within the whole year of 2016. We present benchmark results on several popular VPR algorithms showing that side views are significantly more challenging for current VPR methods while the influence of data anonymization is almost negligible, together with our hypothetical explanations and in-depth analysis.},
  keywords={Location awareness;Visualization;Data privacy;Navigation;Databases;Sociology;Imaging},
  doi={10.1109/IROS51168.2021.9636640},
  ISSN={2153-0866},
  month={Sep.},
  note={* indicates equal contribution},
  selected={true},
  html={https://ieeexplore.ieee.org/abstract/document/9636640},
  pdf={NYU-VPR_Long-Term_Visual_Place_Recognition_Benchmark_with_View_Direction_and_Data_Anonymization_Influences.pdf},
  google_scholar_id={u5HHmVD_uO8C}
}

@misc{chai2024amexandroidmultiannotationexpo,
  title={AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents}, 
  author={Yuxiang Chai* and Siyuan Huang* and Yazhe Niu and Han Xiao and Liang Liu and Dingyu Zhang and Peng Gao and Shuai Ren and Hongsheng Li},
  abstract={AI agents have drawn increasing attention mostly on their ability to perceive environments, understand tasks, and autonomously achieve goals. To advance research on AI agents in mobile scenarios, we introduce the Android Multi-annotation EXpo (AMEX), a comprehensive, large-scale dataset designed for generalist mobile GUI-control agents. Their capabilities of completing complex tasks by directly interacting with the graphical user interface (GUI) on mobile devices are trained and evaluated with the proposed dataset. AMEX comprises over 104K highresolution screenshots from 110 popular mobile applications, which are annotated at multiple levels. Unlike existing mobile device-control datasets, e.g., MoTIF, AITW, etc., AMEX includes three levels of annotations: GUI interactive element grounding, GUI screen and element functionality descriptions, and complex natural language instructions, each averaging 13 steps with stepwise GUI-action chains. We develop this dataset from a more instructive and detailed perspective, complementing the general settings of existing datasets. Additionally, we develop a baseline model SPHINX Agent and compare its performance across state-of-the-art agents trained on other datasets. To facilitate further research, we open-source our dataset, models, and relevant evaluation tools. The project is available at https://yuxiangchai.github.io/AMEX/.},
  year={2024},
  eprint={2407.17490},
  archivePrefix={arXiv},
  primaryClass={cs.HC},
  note={* indicates equal contribution},
  html={https://arxiv.org/abs/2407.17490}, 
  selected={true},
  pdf={AMEX_Android_Multi-annotation_Expo_Dataset_for_Mobile_GUI_Agents.pdf},
  google_scholar_id={u-x6o8ySG0sC}
}

@misc{chai2025a3androidagentarena,
  title={A3: Android Agent Arena for Mobile GUI Agents}, 
  author={Yuxiang Chai and Hanhao Li and Jiayu Zhang and Liang Liu and Guozhi Wang and Shuai Ren and Siyuan Huang and Hongsheng Li},
  year={2025},
  eprint={2501.01149},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  url={https://arxiv.org/abs/2501.01149},   
  selected={true},
  google_scholar_id={d1gkVwhDpl0C},
}

@article{202501.0413,
	doi = {10.20944/preprints202501.0413.v1},
	url = {https://doi.org/10.20944/preprints202501.0413.v1},
	year = 2025,
	month = {January},
	publisher = {Preprints},
	author = {William Liu and Liang Liu and Yaxuan Guo and Han Xiao and Weifeng Lin and Yuxiang Chai and Shuai Ren and Xiaoyu Liang and Linghao Li and Wenhao Wang and Tianze Wu and Yong Liu and Hao Wang and Hongsheng Li and Guanjing Xiong},
	title = {LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects},
	journal = {Preprints},
  google_scholar_id={9yKSN-GCB0IC},
  selected={true},
}

@article{lu2025ui,
  title={UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning},
  author={Lu*, Zhengxi and Chai*, Yuxiang and Guo, Yaxuan and Yin, Xi and Liu, Liang and Wang, Hao and Xiong, Guanjing and Li, Hongsheng},
  journal={arXiv preprint arXiv:2503.21620},
  year={2025},
  google_scholar_id={2osOgNQ5qMEC},
  url={https://arxiv.org/pdf/2503.21620},
  note={* indicates equal contribution},
  selected={true},
}